{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Each word in each document is drawn from one of the topics, where the selected topic is chosen from the per-document distribution over the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Converting the given set of documents of words to a suitable bag of words format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets assume only set of stop words are 'for a of the and to in'\n",
    "stopwords = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords] for document in documents]\n",
    "\n",
    "#Removing words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for document in texts:\n",
    "\tfor token in document:\n",
    "\t\tfrequency[token] += 1\n",
    "\n",
    "texts = [[token for token in document if frequency[token] > 1] for document in texts]\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(\"deerwester.dict\")\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction COMPuter\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split()) # to bag of words representation [similar as above achieved through pre-processing]\n",
    "print(new_vec) #list of tuples -> id, frequency of the word in the new doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(document) for document in texts]\n",
    "corpora.MmCorpus.serialize('deerwester.mm', corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "Latent Dirichlet Allocation Model for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (3, 1), (5, 1), (7, 1)],\n",
       " [(0, 2), (1, 1), (3, 1), (8, 1)],\n",
       " [(1, 1), (3, 1), (6, 1), (9, 1)],\n",
       " [(0, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (11, 1), (13, 1)],\n",
       " [(0, 1), (10, 1)],\n",
       " [(0, 1), (10, 1), (11, 1), (14, 1)],\n",
       " [(13, 1), (14, 1)],\n",
       " [(0, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume tiny corpus with preprocessed data\n",
    "texts = [['bank','river','shore','water'],\n",
    "        ['river','water','flow','fast','tree'],\n",
    "        ['bank','water','fall','flow'],\n",
    "        ['bank','bank','water','rain','river'],\n",
    "        ['river','water','mud','tree'],\n",
    "        ['money','transaction','bank','finance'],\n",
    "        ['bank','borrow','money'], \n",
    "        ['bank','finance'],\n",
    "        ['finance','money','sell','bank'],\n",
    "        ['borrow','sell'],\n",
    "        ['bank','loan','sell']]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) #setting seed as 1 to produce same random results all the time\n",
    "model = LdaModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.207*\"bank\" + 0.100*\"water\" + 0.089*\"river\" + 0.088*\"sell\" + 0.067*\"borrow\" + 0.064*\"finance\" + 0.062*\"money\" + 0.053*\"tree\" + 0.045*\"flow\" + 0.044*\"rain\" + 0.042*\"fast\" + 0.038*\"loan\" + 0.033*\"shore\" + 0.025*\"mud\" + 0.022*\"fall\" + 0.021*\"transaction\"'),\n",
       " (1,\n",
       "  '0.142*\"bank\" + 0.116*\"water\" + 0.090*\"river\" + 0.084*\"money\" + 0.081*\"finance\" + 0.064*\"flow\" + 0.055*\"transaction\" + 0.055*\"tree\" + 0.053*\"fall\" + 0.050*\"mud\" + 0.050*\"sell\" + 0.039*\"shore\" + 0.036*\"borrow\" + 0.033*\"loan\" + 0.028*\"fast\" + 0.025*\"rain\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.086268626), (1, 0.098355845)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_term_topics(\"water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 1]), (3, [0, 1])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_water = ['bank','water','bank']\n",
    "bow_finance = ['bank','finance','bank']\n",
    "\n",
    "bow = model.id2word.doc2bow(bow_water) # convert to bag of words format first\n",
    "doc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True)\n",
    "\n",
    "word_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what does that output mean? It means that like `word_type 1`, our `word_type 3`, which is the word bank, is more likely to be in `topic_0` than `topic_1`.\n",
    "\n",
    "You must have noticed that while we unpacked into `doc_topics` and `word_topics`, there is another variable - `phi_values`. Like the name suggests, phi_values contains the phi values for each topic for that particular word, scaled by feature length. Phi is essentially the probability of that word in that document belonging to a particular topic. The next few lines should illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(0, 1.8300905), (1, 0.16990817)]),\n",
       " (3, [(0, 0.8581231), (1, 0.14187525)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `word_type 0` has the following `phi_values` for each of the topics. What is interesting to note is `word_type 3` - because it has 2 occurrences (i.e, the word bank appears twice in the bow), we can see that the scaling by feature length is very evident. The sum of the phi_values is 2, and not 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 1]), (10, [0, 1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = model.id2word.doc2bow(bow_finance) # convert to bag of words format first\n",
    "doc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True)\n",
    "\n",
    "word_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.73633283), (1, 0.26366717)]\n",
      "Word topics: [(0, [0, 1]), (1, [0, 1]), (2, [0, 1]), (3, [0, 1])]\n",
      "Phi values: [(0, [(0, 0.8527052), (1, 0.14729404)]), (1, [(0, 0.7954733), (1, 0.20452493)]), (2, [(0, 0.77095807), (1, 0.22903538)]), (3, [(0, 0.7647523), (1, 0.23524614)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.75393915), (1, 0.24606086)]\n",
      "Word topics: [(1, [0, 1]), (3, [0, 1]), (4, [0, 1]), (5, [0, 1]), (6, [0, 1])]\n",
      "Phi values: [(1, [(0, 0.8067087), (1, 0.1932895)]), (3, [(0, 0.77720284), (1, 0.22279577)]), (4, [(0, 0.9071241), (1, 0.09287066)]), (5, [(0, 0.72941846), (1, 0.2705778)]), (6, [(0, 0.80554974), (1, 0.19444677)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.1703983), (1, 0.8296017)]\n",
      "Word topics: [(0, [1, 0]), (3, [1, 0]), (5, [1, 0]), (7, [1, 0])]\n",
      "Phi values: [(0, [(0, 0.15414658), (1, 0.84585243)]), (3, [(0, 0.09283457), (1, 0.90716404)]), (5, [(0, 0.07328669), (1, 0.9267104)]), (7, [(0, 0.031027097), (1, 0.96896887)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.8758601), (1, 0.12413992)]\n",
      "Word topics: [(0, [0, 1]), (1, [0, 1]), (3, [0, 1]), (8, [0, 1])]\n",
      "Phi values: [(0, [(0, 1.9142816), (1, 0.08571696)]), (1, [(0, 0.9375137), (1, 0.06248459)]), (3, [(0, 0.9261476), (1, 0.07385102)]), (8, [(0, 0.97765744), (1, 0.022337971)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.17125681), (1, 0.82874316)]\n",
      "Word topics: [(1, [1, 0]), (3, [1, 0]), (6, [1, 0]), (9, [1, 0])]\n",
      "Phi values: [(1, [(0, 0.11006489), (1, 0.8899332)]), (3, [(0, 0.093688674), (1, 0.90631)]), (6, [(0, 0.109341264), (1, 0.8906552)]), (9, [(0, 0.042480744), (1, 0.9575149)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.16402593), (1, 0.83597404)]\n",
      "Word topics: [(0, [1, 0]), (10, [1, 0]), (11, [1, 0]), (12, [1, 0])]\n",
      "Phi values: [(0, [(0, 0.14435509), (1, 0.85564387)]), (10, [(0, 0.0796064), (1, 0.9203915)]), (11, [(0, 0.07176129), (1, 0.92823666)]), (12, [(0, 0.023786563), (1, 0.97620964)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.8052418), (1, 0.19475818)]\n",
      "Word topics: [(0, [0, 1]), (11, [0, 1]), (13, [0, 1])]\n",
      "Phi values: [(0, [(0, 0.9217698), (1, 0.078229435)]), (11, [(0, 0.84373283), (1, 0.15626447)]), (13, [(0, 0.9561039), (1, 0.04389328)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.6528348), (1, 0.3471652)]\n",
      "Word topics: [(0, [0, 1]), (10, [0, 1])]\n",
      "Phi values: [(0, [(0, 0.79454714), (1, 0.20545205)]), (10, [(0, 0.6647256), (1, 0.33527175)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.7917555), (1, 0.20824447)]\n",
      "Word topics: [(0, [0, 1]), (10, [0, 1]), (11, [0, 1]), (14, [0, 1])]\n",
      "Phi values: [(0, [(0, 0.9003983), (1, 0.09960102)]), (10, [(0, 0.8225213), (1, 0.17747611)]), (11, [(0, 0.8055402), (1, 0.19445714)]), (14, [(0, 0.93105084), (1, 0.06894723)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.80841804), (1, 0.19158193)]\n",
      "Word topics: [(13, [0, 1]), (14, [0, 1])]\n",
      "Phi values: [(13, [(0, 0.9664923), (1, 0.033504896)]), (14, [(0, 0.95886075), (1, 0.04113719)])]\n",
      " \n",
      "-------------- \n",
      "\n",
      "New Document \n",
      "\n",
      "Document topics: [(0, 0.84000635), (1, 0.15999365)]\n",
      "Word topics: [(0, [0, 1]), (14, [0, 1]), (15, [0, 1])]\n",
      "Phi values: [(0, [(0, 0.94806826), (1, 0.05193099)]), (14, [(0, 0.9646261), (1, 0.035372064)]), (15, [(0, 0.94757134), (1, 0.052422978)])]\n",
      " \n",
      "-------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_topics = model.get_document_topics(corpus, per_word_topics=True)\n",
    "\n",
    "for doc_topics, word_topics, phi_values in all_topics:\n",
    "    print('New Document \\n')\n",
    "    print('Document topics:', doc_topics)\n",
    "    print('Word topics:', word_topics)\n",
    "    print('Phi values:', phi_values)\n",
    "    print(\" \")\n",
    "    print('-------------- \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6863e040e8981097dbf29d10dc72ab85d80010d98b5512de1073715d1caccbff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
